Batch Normalization
	배치정규화
	import tensorflow
사용법:tf.layers.batch_normalization(
    inputs,
    axis=-1,
    momentum=0.99,
    epsilon=0.001,
    center=True,
    scale=True,
    beta_initializer=tf.zeros_initializer(),
    gamma_initializer=tf.ones_initializer(),
    moving_mean_initializer=tf.zeros_initializer(),
    moving_variance_initializer=tf.ones_initializer(),
    beta_regularizer=None,
    gamma_regularizer=None,
    beta_constraint=None,
    gamma_constraint=None,
    training=False,
    trainable=True,
    name=None,
    reuse=None,
    renorm=False,
    renorm_clipping=None,
    renorm_momentum=0.99,
    fused=None,
    virtual_batch_size=None,
    adjustment=None)
인자:
inputs: 텐서 입력.
axis: int정규화해야하는 축 (일반적으로 피처 축) 예를 들어, 후 Convolution2D와 층 data_format="channels_first"설정 axis=1에서 BatchNormalization.
momentum: 이동 평균의 모멘텀.
epsilon: 0으로 나누지 않도록 분산에 작은 부동 소수점이 추가되었습니다.
center: True 인 경우 beta정규화 된 텐서에 오프셋을 추가 합니다. False이면 beta 무시됩니다.
scale: 참이면을 곱하십시오 gamma. False이면 gamma사용되지 않습니다. 다음 레이어가 선형 인 경우 (예 :), 다음 레이어 nn.relu에서 스케일링을 수행 할 수 있으므로 비활성화 할 수 있습니다.
beta_initializer: 베타 가중치의 이니셜 라이저.
gamma_initializer: 감마 무게 초기화.
moving_mean_initializer: 이동 평균의 이니셜 라이저.
moving_variance_initializer: 이동 분산을위한 이니셜 라이저.
beta_regularizer: 베타 웨이트 용 옵션 정규 화제.
gamma_regularizer: 감마 무게를위한 선택적인 정규 화제.
beta_constraint: beta 업데이트 후 가중치에 적용 할 선택적 투영 함수 Optimizer(예 : 레이어 가중치에 대한 표준 구속 조건 또는 값 제약 조건을 구현하는 데 사용). 이 함수는 프로젝션되지 않은 변수를 입력으로 가져 와서 프로젝션 된 변수 (같은 모양이어야 함)를 반환해야합니다. 비동기식 분산 교육을 수행 할 때 제약 조건을 사용하는 것이 안전하지 않습니다.
gamma_constraint:에 gamma의해 업데이트 된 후 가중치에 적용 할 선택적 투영 기능 Optimizer.
training: Python 부울 또는 TensorFlow 부울 스칼라 텐서 (예 : 플레이스 홀더)입니다. 훈련 모드 (현재 배치의 통계로 정규화) 또는 추론 모드 (이동 통계로 정규화)에서 출력을 반환할지 여부 참고 :이 매개 변수를 올바르게 설정하지 않으면 교육 / 추론이 제대로 작동하지 않습니다.
trainable: True그래프 컬렉션에 변수를 추가하는 경우 부울 입니다 GraphKeys.TRAINABLE_VARIABLES(tf.Variable 참조).
name: 문자열, 레이어 이름.
reuse: 부울, 같은 이름으로 이전 레이어의 가중치를 재사용할지 여부.
renorm: 일괄 재 정규화 사용 여부 (https://arxiv.org/abs/1702.03275) 이것은 훈련 중에 추가 변수를 추가합니다. 이 매개 변수의 값에 대해서는 추론이 동일합니다.
renorm_clipping: 'rmax', 'rmin', 'dmax'키를 Tensorsrenorm 보정을 클립하는 데 사용되는 스칼라에 매핑 할 수있는 사전입니다 . 보정 (r, d)은로 사용되며 [rmin, rmax] 및 [-dmax, dmax] corrected_value = normalized_value * r + d로 r클리핑됩니다 d. 누락 된 rmax, rmin, dmax는 각각 inf, 0, inf로 설정됩니다.
renorm_momentum: 운동량 및 표준 편차를 renorm으로 업데이트하는 데 사용되는 운동량. 와 달리 momentum, 이것은 훈련에 영향을 미치며 너무 작거나 (소음이 추가 될) 너무 크지 않아야합니다 (부실한 추정치 제공). 참고 momentum여전히 수단을 얻기 위해 적용 추론에 대한 편차를한다.
fused: None또는 True경우 가능하면 더 빠르고 융합 된 구현을 사용하십시오. 인 경우 False시스템 권장 구현을 사용하십시오.
virtual_batch_size: int. 기본적으로 virtual_batch_sizeis None는 일괄 처리 정규화가 전체 일괄 처리에서 수행됨을 의미합니다. 시 virtual_batch_size아닌 None, 대신에 각 (공유 감마, 베타 및 이동 통계) 별도로 표준화 가상 서브 일괄 작성 "고스트 배치 정규화"를 수행한다. 실행 중에 실제 배치 크기를 나누어야합니다.
adjustment: Tensor입력 텐서의 (동적) 모양을 포함하고 훈련 중에 만 정규화 된 값 (감마 및 베타 이전)에 적용하기 위해 한 쌍 (스케일, 바이어스)을 반환하는 함수입니다. 예를 들어, axis ==-1 인 adjustment = lambda shape: ( tf.random.uniform(shape[-1:], 0.93, 1.07), tf.random.uniform(shape[-1:], -0.1, 0.1)) 경우 정규화 된 값을 최대 7 % 위 또는 아래로 스케일링 한 다음 결과를 최대 0.1 (각 피처에 대해 독립적 인 스케일링 및 바이어스를 사용하지만 모든 예제에서 공유)로 최대 이동합니다. 감마 및 / 또는 베타를 적용합니다. 인 경우 None조정이 적용되지 않습니다. virtual_batch_size가 지정된 경우 지정할 수 없습니다.


cross validation
	단순 교차검증 cross_val_score
	from sklearn.model_selection import cross_val_score
사용법 : cross_val_score(
estimator, 
X, 
y=None, 
groups=None, 
scoring=None, 
cv='warn', 
n_jobs=None, 
verbose=0, 
fit_params=None, 
pre_dispatch='2*n_jobs', 
error_score='raise-deprecating')

인자 : 
estimator : estimator object implementing ‘fit’/데이터를 맞추는데 사용할 객체
X : array-like/훈련데이터
y : array-like, optional, default: None/타깃
groups : array-like, with shape (n_samples,), optional/배열모양
scoring : string, callable or None, optional, default: None
cv : int, cross-validation generator or an iterable, optional/폴드의 수


2.	계층별 k-겹 교차검증
	from sklearn.model_selection import StratifiedKFold

사용법 : StratifiedKFold( 
n_splits = 'warn', 
shuffle = False, 
random_state = None )

인자 : 
n_splits : int, default=3 / 몇 개로 분할할지 정하는 매개변수
shuffle : boolean, optional / True일경우 폴드를 나누기 전에 섞음
random_state : int, RandomState instance or None, optional, default=None / shuffle== True인 경우만 사용, 난수생성

3.	KFold 상세조정
	from sklearn.model_selection import KFold

사용법 : KFold(
n_splits='warn', 
shuffle=False, 
random_state=None)

인자 : 
위와 같음

4.	임의분할 교차검증
	from sklearn.model_selection import ShuffleSplit

사용법 : ShuffleSplit(
n_splits='warn', 
shuffle=False, 
random_state=None)

인자 : 위와같음


Weight decay
	가중치 감소
	from tensorflow as tf
사용법 : tf.nn.l2_loss(t, name=None)
인자 :
t:half, bfloat16, float32, float64. 일반적으로 2 차원.
name: 조작 이름


Transfer learning
ex)vgg16사용
from keras.applications.vgg16 import VGG16
model = VGG16VGG16(
include_top=True, 
weights='imagenet', 
input_tensor=None, 
input_shape=None, 
pooling=None, 
classes=1000)

인자 : 
include_top: 네트워크 상단에 3개의 완전히 연결된 레이어를 포함할지 여부.
weights: None(임의 초기화) 또는 'imagenet'(ImageNet의 사전 훈련 ) 중 하나
input_tensor: layers.Input()모델의 이미지 입력으로 사용할 선택적 Keras 텐서.
input_shape: 옵션 모양 튜플, include_topis 인 경우에만 지정 False(그렇지 않으면 입력 모양이 (224, 224, 3)( 'channels_last'데이터 형식) 또는 (3, 224, 224)( 'channels_first'데이터 형식)이어야 함) 정확히 3 개의 입력 채널이 있어야하며 너비와 높이는 32보다 작아야합니다 예를 들어 (200, 200, 3)하나의 유효한 값이됩니다.
pooling: 
None : 모델의 출력이 마지막 컨볼루션 블록의 4D 텐서 출력이됨을 의미.
'avg' : 글로벌 평균 풀링이 마지막 컨 루션 블록의 출력에 적용되므로 모델의 출력은 2D 텐서가됩니다.
'max' 전역 최대풀링이 적용됨을 의미합니다.
classes: 이미지를 분류 할 클래스의 선택적 수로, include_top이 True인 경우에만 지정되고 weights인수가 지정되지 않은 경우에만 지정됩니다.


Ensemble
1.
from sklearn.ensemble import VotingClassifier
voting_clf = VotingClassifier(
    estimators=[('이름', 모델), ('이름', 모델), ('이름', 모델)],
    voting='hard/soft')
voting_clf.fit(X_train, y_train)
##
from sklearn.metrics import accuracy_score
?
for clf in (모델, 모델, 모델, voting_clf):
    clf.fit(X_train, y_train)
    y_pred = clf.predict(X_test)
    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))

인자 : 
estimators = 지정할 이름과 사용할 모델을 지정한다.
voting = hard : 각 모델들의 예측값을 가지고 다수결 투표를 통해 예측
	 soft : 각 모델들의 예측값의 확률을 가지고 평균을 구한 뒤, 평균이 가장 높은 클래스로 예측(간접투표)


2. Bagging
from sklearn.ensemble import BaggingClassifier

bag_clf = BaggingClassifier(
base_estimator=None, #디시젼트리 사용
n_estimators=10, 
max_samples=1.0, 
max_features=1.0, 
)

bag_clf.fit(X_train, y_train)
y_pred = bag_clf.predict(X_test)
?
print('Accuracy =', accuracy_score(y_test, y_pred))

인자 : 
base_estimator : 모델 입력 (default=None)
n_estimators : int, optional (default=10)/앙상블 추정기의 수
max_samples : int or float, optional (default=1.0)/X에서 추출할 샘플 수
max_features : int or float, optional (default=1.0)/추정기를 훈련시키기 위해 X에서 그릴 기능의 수


3. boosting
from sklearn.ensemble import AdaBoostClassifier
AdaBoostClassifier( 
base_estimator = None, 
n_estimators = 50, 
learning_rate = 1.0, 
algorithm = 'SAMME.R', 
random_state = None )

인자 : 
base_estimator : object, optional (default=None)
앙상블이 만들어지는 기본 추정기. None인 경우 DecisionTreeClassifier(max_depth=1)
n_estimators : integer, optional (default=50)
부스팅이 종료되는 최대 추정량
learning_rate : float, optional (default=1.)
algorithm : {'SAMME', 'SAMME.R'}, optional (default='SAMME.R')
SAMME.R 알고리즘은 일반적으로 SAMME보다 빠르게 수렴되므로 부스팅 반복 횟수가 줄어 테스트 오류가 줄어든다
random_state : int, RandomState instance or None, optional (default=None)
난수생성


DropOut
tf.nn.dropout(
    x,
    rate,
    noise_shape=None,
    seed=None,
    name=None
)
인자 :
x: 부동 소수점 텐서.
rate: 텐서 x와 같은 유형의 스칼라. 각 요소가 삭제 될 확률. 예를 들어 rate = 0.1을 설정하면 입력 요소의 10 %가 삭제됩니다.
noise_shape: 무작위로 생성 된 유지 / 삭제 플래그의 모양을 나타내는 1-D Tensor.
seed: 정수값, 같은 시드값이면 같은 텐서들이 삭제
name:이 작업의 이름.


Bayesian Optimization



